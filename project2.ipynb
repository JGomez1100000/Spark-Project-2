{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/javier/.local/lib/python3.10/site-packages (3.2)\n",
      "Requirement already satisfied: pyspark in /home/javier/.local/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: findspark in /home/javier/.local/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/javier/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget pysparkâ€¯ findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 13:54:24 WARN Utils: Your hostname, javier-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.81.128 instead (on interface ens33)\n",
      "24/04/25 13:54:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/25 13:54:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a SparkContext object\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a Spark Session\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'employees.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/employees.csv\"\n",
    "wget.download(link_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a Spark DataFrame from the CSV data\n",
    "employees_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Emp_No: integer (nullable = true)\n",
      " |-- Emp_Name: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "employees_df.createOrReplaceTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to fetch solely the records from the View where the age exceeds 30\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM employees\n",
    "WHERE age > 30\n",
    "\"\"\"\n",
    "\n",
    "df_sql1 = spark.sql(sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+---+----------+\n",
      "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
      "+------+-----------+------+---+----------+\n",
      "|   199|    Douglas|  2600| 34|     Sales|\n",
      "|   200|   Jennifer|  4400| 36| Marketing|\n",
      "|   201|    Michael| 13000| 32|        IT|\n",
      "|   202|        Pat|  6000| 39|        HR|\n",
      "|   203|      Susan|  6500| 36| Marketing|\n",
      "|   205|    Shelley| 12008| 33|   Finance|\n",
      "|   206|    William|  8300| 37|        IT|\n",
      "|   100|     Steven| 24000| 39|        IT|\n",
      "|   102|        Lex| 17000| 37| Marketing|\n",
      "|   103|  Alexander|  9000| 39| Marketing|\n",
      "|   104|      Bruce|  6000| 38|        IT|\n",
      "|   105|      David|  4800| 39|        IT|\n",
      "|   106|      Valli|  4800| 38|     Sales|\n",
      "|   107|      Diana|  4200| 35|     Sales|\n",
      "|   109|     Daniel|  9000| 35|        HR|\n",
      "|   110|       John|  8200| 31| Marketing|\n",
      "|   111|     Ismael|  7700| 32|        IT|\n",
      "|   112|Jose Manuel|  7800| 34|        HR|\n",
      "|   113|       Luis|  6900| 34|     Sales|\n",
      "|   116|     Shelli|  2900| 37|   Finance|\n",
      "+------+-----------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|department|average_salary|\n",
      "+----------+--------------+\n",
      "|     Sales|        5493.0|\n",
      "|        HR|        5838.0|\n",
      "|   Finance|        5731.0|\n",
      "| Marketing|        6633.0|\n",
      "|        IT|        7400.0|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL query to calculate the average salary of employees grouped by department\n",
    "sql_query = \"\"\"\n",
    "SELECT department, ROUND(AVG(salary)) AS average_salary\n",
    "FROM employees\n",
    "GROUP BY department\n",
    "\"\"\"\n",
    "\n",
    "df_sql2 = spark.sql(sql_query)\n",
    "df_sql2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+---+----------+\n",
      "|Emp_No|Emp_Name|Salary|Age|Department|\n",
      "+------+--------+------+---+----------+\n",
      "|   198|  Donald|  2600| 29|        IT|\n",
      "|   201| Michael| 13000| 32|        IT|\n",
      "|   206| William|  8300| 37|        IT|\n",
      "|   100|  Steven| 24000| 39|        IT|\n",
      "|   104|   Bruce|  6000| 38|        IT|\n",
      "|   105|   David|  4800| 39|        IT|\n",
      "|   111|  Ismael|  7700| 32|        IT|\n",
      "|   129|   Laura|  3300| 38|        IT|\n",
      "|   132|      TJ|  2100| 34|        IT|\n",
      "|   136|   Hazel|  2200| 29|        IT|\n",
      "+------+--------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply a filter to select records where the department is 'IT'\n",
    "\n",
    "filtered_df = employees_df.filter(employees_df.Department == 'IT')\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+----------------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department|SalaryAfterBonus|\n",
      "+------+---------+------+---+----------+----------------+\n",
      "|   198|   Donald|  2600| 29|        IT|          2860.0|\n",
      "|   199|  Douglas|  2600| 34|     Sales|          2860.0|\n",
      "|   200| Jennifer|  4400| 36| Marketing|          4840.0|\n",
      "|   201|  Michael| 13000| 32|        IT|         14300.0|\n",
      "|   202|      Pat|  6000| 39|        HR|          6600.0|\n",
      "|   203|    Susan|  6500| 36| Marketing|          7150.0|\n",
      "|   204|  Hermann| 10000| 29|   Finance|         11000.0|\n",
      "|   205|  Shelley| 12008| 33|   Finance|         13209.0|\n",
      "|   206|  William|  8300| 37|        IT|          9130.0|\n",
      "|   100|   Steven| 24000| 39|        IT|         26400.0|\n",
      "|   101|    Neena| 17000| 27|     Sales|         18700.0|\n",
      "|   102|      Lex| 17000| 37| Marketing|         18700.0|\n",
      "|   103|Alexander|  9000| 39| Marketing|          9900.0|\n",
      "|   104|    Bruce|  6000| 38|        IT|          6600.0|\n",
      "|   105|    David|  4800| 39|        IT|          5280.0|\n",
      "|   106|    Valli|  4800| 38|     Sales|          5280.0|\n",
      "|   107|    Diana|  4200| 35|     Sales|          4620.0|\n",
      "|   108|    Nancy| 12008| 28|     Sales|         13209.0|\n",
      "|   109|   Daniel|  9000| 35|        HR|          9900.0|\n",
      "|   110|     John|  8200| 31| Marketing|          9020.0|\n",
      "+------+---------+------+---+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add a new column \"SalaryAfterBonus\" with 10% bonus added to the original salary\n",
    "\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "employees_df_with_bonus = employees_df.withColumn(\"SalaryAfterBonus\", round(col(\"salary\") * 1.10))\n",
    "\n",
    "employees_df_with_bonus.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|age|MaxSalary|\n",
      "+---+---------+\n",
      "| 26|     3600|\n",
      "| 27|    17000|\n",
      "| 28|    12008|\n",
      "| 29|    10000|\n",
      "| 30|     8000|\n",
      "| 31|     8200|\n",
      "| 32|    13000|\n",
      "| 33|    12008|\n",
      "| 34|     7800|\n",
      "| 35|     9000|\n",
      "| 36|     7900|\n",
      "| 37|    17000|\n",
      "| 38|     6000|\n",
      "| 39|    24000|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group data by age and calculate the maximum salary for each age group\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "max_salary_by_age = employees_df.groupBy(\"age\").agg(max(\"salary\").alias(\"MaxSalary\")).orderBy(\"age\")\n",
    "\n",
    "max_salary_by_age.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+---------+------+---+----------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department| Emp_Name|Salary|Age|Department|\n",
      "+------+---------+------+---+----------+---------+------+---+----------+\n",
      "|   198|   Donald|  2600| 29|        IT|   Donald|  2600| 29|        IT|\n",
      "|   199|  Douglas|  2600| 34|     Sales|  Douglas|  2600| 34|     Sales|\n",
      "|   200| Jennifer|  4400| 36| Marketing| Jennifer|  4400| 36| Marketing|\n",
      "|   201|  Michael| 13000| 32|        IT|  Michael| 13000| 32|        IT|\n",
      "|   202|      Pat|  6000| 39|        HR|      Pat|  6000| 39|        HR|\n",
      "|   203|    Susan|  6500| 36| Marketing|    Susan|  6500| 36| Marketing|\n",
      "|   204|  Hermann| 10000| 29|   Finance|  Hermann| 10000| 29|   Finance|\n",
      "|   205|  Shelley| 12008| 33|   Finance|  Shelley| 12008| 33|   Finance|\n",
      "|   206|  William|  8300| 37|        IT|  William|  8300| 37|        IT|\n",
      "|   100|   Steven| 24000| 39|        IT|   Steven| 24000| 39|        IT|\n",
      "|   101|    Neena| 17000| 27|     Sales|    Neena| 17000| 27|     Sales|\n",
      "|   102|      Lex| 17000| 37| Marketing|      Lex| 17000| 37| Marketing|\n",
      "|   103|Alexander|  9000| 39| Marketing|Alexander|  9000| 39| Marketing|\n",
      "|   104|    Bruce|  6000| 38|        IT|    Bruce|  6000| 38|        IT|\n",
      "|   105|    David|  4800| 39|        IT|    David|  4800| 39|        IT|\n",
      "|   106|    Valli|  4800| 38|     Sales|    Valli|  4800| 38|     Sales|\n",
      "|   107|    Diana|  4200| 35|     Sales|    Diana|  4200| 35|     Sales|\n",
      "|   108|    Nancy| 12008| 28|     Sales|    Nancy| 12008| 28|     Sales|\n",
      "|   109|   Daniel|  9000| 35|        HR|   Daniel|  9000| 35|        HR|\n",
      "|   110|     John|  8200| 31| Marketing|     John|  8200| 31| Marketing|\n",
      "+------+---------+------+---+----------+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the DataFrame with itself based on the \"Emp_No\" column\n",
    "\n",
    "joined_df = employees_df.alias(\"df1\").join(employees_df.alias(\"df2\"), \"Emp_No\")\n",
    "\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|AverageAge|\n",
      "+----------+\n",
      "|     33.56|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average age of employees\n",
    "\n",
    "from pyspark.sql.functions import avg \n",
    "\n",
    "average_age = employees_df.select(avg(\"Age\").alias(\"AverageAge\"))\n",
    "\n",
    "average_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|TotalSalary|\n",
      "+----------+-----------+\n",
      "|     Sales|      71408|\n",
      "|        HR|      46700|\n",
      "|   Finance|      57308|\n",
      "| Marketing|      59700|\n",
      "|        IT|      74000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the total salary for each department\n",
    "from pyspark.sql.functions import sum \n",
    "\n",
    "total_salary_per_department = employees_df.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"TotalSalary\"))\n",
    "\n",
    "\n",
    "total_salary_per_department.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department|\n",
      "+------+---------+------+---+----------+\n",
      "|   137|   Renske|  3600| 26| Marketing|\n",
      "|   101|    Neena| 17000| 27|     Sales|\n",
      "|   114|      Den| 11000| 27|   Finance|\n",
      "|   108|    Nancy| 12008| 28|     Sales|\n",
      "|   130|    Mozhe|  2800| 28| Marketing|\n",
      "|   126|    Irene|  2700| 28|        HR|\n",
      "|   204|  Hermann| 10000| 29|   Finance|\n",
      "|   115|Alexander|  3100| 29|   Finance|\n",
      "|   134|  Michael|  2900| 29|     Sales|\n",
      "|   198|   Donald|  2600| 29|        IT|\n",
      "|   140|   Joshua|  2500| 29|   Finance|\n",
      "|   136|    Hazel|  2200| 29|        IT|\n",
      "|   120|  Matthew|  8000| 30|        HR|\n",
      "|   110|     John|  8200| 31| Marketing|\n",
      "|   127|    James|  2400| 31|        HR|\n",
      "|   201|  Michael| 13000| 32|        IT|\n",
      "|   111|   Ismael|  7700| 32|        IT|\n",
      "|   119|    Karen|  2500| 32|   Finance|\n",
      "|   205|  Shelley| 12008| 33|   Finance|\n",
      "|   124|    Kevin|  5800| 33| Marketing|\n",
      "+------+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by age in ascending order and then by salary in descending order\n",
    "\n",
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "sorted_df = employees_df.orderBy(asc(\"Age\"), desc(\"Salary\"))\n",
    "\n",
    "sorted_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|Department|EmployeeCount|\n",
      "+----------+-------------+\n",
      "|     Sales|           13|\n",
      "|        HR|            8|\n",
      "|   Finance|           10|\n",
      "| Marketing|            9|\n",
      "|        IT|           10|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of employees in each department\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "dept_employee_count = employees_df.groupBy(\"Department\").agg(count(\"Emp_No\").alias(\"EmployeeCount\"))\n",
    "\n",
    "dept_employee_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+---+----------+\n",
      "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
      "+------+-----------+------+---+----------+\n",
      "|   198|     Donald|  2600| 29|        IT|\n",
      "|   199|    Douglas|  2600| 34|     Sales|\n",
      "|   110|       John|  8200| 31| Marketing|\n",
      "|   112|Jose Manuel|  7800| 34|        HR|\n",
      "|   130|      Mozhe|  2800| 28| Marketing|\n",
      "|   133|      Jason|  3300| 38|     Sales|\n",
      "|   139|       John|  2700| 36|     Sales|\n",
      "|   140|     Joshua|  2500| 29|   Finance|\n",
      "+------+-----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply a filter to select records where the employee's name contains the letter 'o'\n",
    "\n",
    "filtered_df = employees_df.filter(employees_df[\"Emp_Name\"].like(\"%o%\"))\n",
    "\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
